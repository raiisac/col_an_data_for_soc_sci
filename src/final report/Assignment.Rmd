---
documentclass: article
fontsize: 10pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    fig_caption: yes
    toc: false
    latex_engine: xelatex
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
bibliography: references.bib  
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, appendix=TRUE}
#---------------------------load all packages----------------------------------#
#General packages
library(tidyverse)       #Data wrangling and ggplot
library(rprojroot)       #Relative file paths for reproducibility
library(kableExtra)      #Pretty "kable" tables
library(git2rdata)       #to efficiently store data on github
library(arrow)           #save and load parquet files
library(patchwork)       #combining ggplot figures

#packages for web scraping
library(webdriver)
library(rvest)
library(furrr)           #webscrape in parallel


#load custom functions
source(find_root_file("src", "final report", "helperfunctions.R",
                      criterion = has_file("col_an_data_for_soc_sci.Rproj")))

knitr::opts_chunk$set(echo = FALSE) # do not show code by default.
knitr::opts_knit$set(
  root.dir = find_root(criterion = has_file("col_an_data_for_soc_sci.Rproj")))

# important option for text analysis
options(stringsAsFactors = F)
```

# Introduction

The Israeli-Palestinian conflict has its origins in the late 19$^{th}$ and early 20$^{th}$ century when Jewish settlers arrived to Ottoman Palestine. In 1947, after the second world war, the United Nations (UN) partition plan officially split Palestine into and Arab state, Jewish state, and Jerusalem. Important aspects of conflict are the Israeli occupation of the West Bank and Gaza strip, Israeli settlements (see also Figure \@ref(fig:territory)), the division of Jerusalem, borders, security and water rights. Over the years, the conflict has escalated to actual wars several times. The UN partition plan led to a lot of violence and, in 1948, the Arab-Israeli war. The six-day war in 1967 and the Yom-Kippur war in 1973 were followed by the first (1987-1993) and second (2000-2005) Intifada. Currently, the conflict is back in the news with an ongoing war between Israel and Hamas. Both parties have their supporters and this research aims to investigate whether a sentiment analysis of Wikipedia pages on the conflict in different languages can unveil underlying positive or negative feelings towards the conflict, Israel and Palestine.

Wikipedia pages are typically written by a community of interested and motivated volunteers. Pages on the most popular/important topics typically exist in many different languages and the content may differ quite substantially, depending on the language. They are typically not a literal translation from one language to the other but are typically written by speakers of the language with some knowledge on the topic at hand. We will focus on the languages spoken by Israeli (Hebrew), the Palestinians (Arabic), some "Western" languages (English, Dutch, French, German, Italian, Spanish) and some Eastern languages (Chinese, Russian, Persian).


```{r territory, echo=FALSE, fig.cap= "Division of Palestinian and Israeli territory over the years (source: https://yalibnan.com/2014/06/27/eu-warns-business-israeli-settlements/)", out.width = "10cm", eval = FALSE}
knitr::include_graphics(
  find_root_file("data", "fig", "vanishing-palestine.jpg",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```


## Literature review

The Israeli-Palestinian conflict has been analysed using sentiment analysis before. In this section, we refer to some inspiring examples. 

The documents on which the sentiment analysis is applied differs over different research articles.  Twitter has likely been the most popular choice [@al2019multi; @imtiaz2022taking; @matalon2021using; @ramdhani2023sentiment; @abusheikh2023digital] but web scraping from Twitter's successor, X, has become very hard so we looked for alternatives.
@al2018bias identify patterns in the press orientation and further in the media bias towards either the Palestinian or Israeli side from press reports and articles.
Similarly, @alsubari2024online base their research on news channel streams to assess channel and user engagement. 

Other examples where opinions are compared in different languages are @abusheikh2023digital and @al2012automatic. The latter is of particular interest since they also base their research on wikipedia pages. 

# Methodology


We plan to follow the following steps:

1. Web scraping and web crawling: The automated download of HTML pages is called **Crawling**. The extraction of the textual data and/or metadata (for example, article date, headlines, author names, article text) from the HTML source code is called **Scraping**. We plan to use he R package [\underline{rvest}](https://cran.r-project.org/web/packages/rvest/index.html) for this tesk, inspired by two tutorials in particular [\underline{here}](https://ghanadatastuff.com/post/webscraping_wikipedia/) and [\underline{here}](https://github.com/tm4ss/tm4ss.github.io/blob/master/Tutorial_1_Web_scraping.Rmd). 
2. Sentiment analysis: Since we want to compare the sentiment of wikipedia pages in different languages, we need a tool that is able to perform sentiment analysis in different languages. The [\underline{Syuzhet}](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html) package currently supports many languages, among which there are Arabic, Chinese, Dutch, English, French, German, Hebrew, Italian, Persian, Russian, and Spanish.
3. A statistical analysis: It is yet unclear which statistical analysis will fit best to answer our main research questions pertaining to the difference in sentiment/opinion on the Israeli-Palestinian conflict, depending on the language of the documents. Finding clusters of languages with similar sentiments seems like a sensible route for now.


# Results

## Web scraping and crawling

We will show all code used to web scrape the relevant Wikipedia pages. However, to efficiently run the notebook, we store and share all web-scraped information on GitHub. This data, if available on your local machine (pull the repository to get it), will be loaded from your hard drive into your R environment in stead of being re-created from scratch. This will save a significant amount of time.

The web scraping starts from **one** Wikipedia page for each language. Starting from the English version of "Israeli-Palestinian conflict" page ([here](https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict)), we manually get the link to all other 10 languages of interest for this page. Starting from these 11 URLs, we get all links to other Wikipedia pages in the same language that are accessible from the "Israeli-Palestinian conflict" page. 

```{r webscraping-geturls, echo = FALSE, eval = !file.exists(find_root_file("data", "all_urls.yml", criterion =  has_file("col_an_data_for_soc_sci.Rproj")))}
if(!file.exists(
  find_root_file("data", "all_urls.yml",
                 criterion =  has_file("col_an_data_for_soc_sci.Rproj")))){
  #webdriver::install_phantomjs() #install the [phantomJS](https://phantomjs.org)
  #headless browser. This needs to be done only once.
  #this dataframe will form the basis 
  urls <- data.frame(
    language = c("English", "Hebrew", "Arabic", "Dutch", "French", "German",
                 "Italian", "Spanish", "Chinese", "Russian", "Persian"),
    language_short = c("Eng", "Heb", "Ara", "Dut", "Fre", "Ger", "Ita", "Spa",
                       "Chi", "Rus", "Per"),
    url = c("https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict",#Eng
          "https://he.wikipedia.org/wiki/%D7%94%D7%A1%D7%9B%D7%A1%D7%95%D7%9A_%D7%94%D7%99%D7%A9%D7%A8%D7%90%D7%9C%D7%99-%D7%A4%D7%9C%D7%A1%D7%98%D7%99%D7%A0%D7%99",#Heb
          "https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%82%D8%B6%D9%8A%D8%A9_%D8%A7%D9%84%D9%81%D9%84%D8%B3%D8%B7%D9%8A%D9%86%D9%8A%D8%A9",#Ara
          "https://nl.wikipedia.org/wiki/Isra%C3%ABlisch-Palestijns_conflict", #Dut
          "https://fr.wikipedia.org/wiki/Conflit_isra%C3%A9lo-palestinien",#Fre
          "https://de.wikipedia.org/wiki/Israelisch-pal%C3%A4stinensischer_Konflikt", #Ger
          "https://it.wikipedia.org/wiki/Conflitto_israelo-palestinese", #Ita
          "https://es.wikipedia.org/wiki/Conflicto_israel%C3%AD-palestino", #Spa
          "https://zh.wikipedia.org/wiki/%E4%BB%A5%E5%B7%B4%E5%86%B2%E7%AA%81",#Chi
          "https://ru.wikipedia.org/wiki/%D0%9F%D0%B0%D0%BB%D0%B5%D1%81%D1%82%D0%B8%D0%BD%D0%BE-%D0%B8%D0%B7%D1%80%D0%B0%D0%B8%D0%BB%D1%8C%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BE%D0%BD%D1%84%D0%BB%D0%B8%D0%BA%D1%82", #Rus
          "https://fa.wikipedia.org/wiki/%D8%AF%D8%B1%DA%AF%DB%8C%D8%B1%DB%8C_%D9%81%D9%84%D8%B3%D8%B7%DB%8C%D9%86_%D9%88_%D8%A7%D8%B3%D8%B1%D8%A7%D8%A6%DB%8C%D9%84"#Per
          ))
  links <- lapply(X = seq_len(nrow(urls)),
                  FUN = function(x) get_all_links(
                    language = urls[x, "language"],
                    language_short = urls[x, "language_short"],
                    url = urls[x, "url"]
                  ))
  links <- bind_rows(links) %>%
    mutate(language = as.factor(language),
           language_short = as.factor(language_short))
  #save the urls such that this data can be loaded locally during knitting.
  write_vc(x = links,
           file = "all_urls",
           root = find_root_file("data",
                                 criterion =
                                   has_file("col_an_data_for_soc_sci.Rproj")))
}
```
```{r webscraping-geturls-load, eval = file.exists(find_root_file("data", "all_urls.yml", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
#load the file locally if it exists
links <- read_vc(
  file = "all_urls",
  root = find_root_file("data",
                        criterion =
                          has_file("col_an_data_for_soc_sci.Rproj"))) %>%
  mutate(language = as.factor(language),
         language_short = as.factor(language_short))
``` 

In total, this search yields `r nrow(links)` URLs for all 11 languages. Figure \@ref(fig:nb-urls) shows the number of linked Wikipedia pages for each of the languages on the left. For comparison, we show the number of (native) speaker on the right since the number of Wikipedia pages will depend a lot on the number of speakers who can write the pages. Taking both into account, we can see that Arabic and English have the most Wikipedia pages linked to the "Israeli-Palestinian conflict" page. Arabic and Hebrew, logically, have much more Wikipedia pages on the issue than what would be expected, given their lower number of native speakers.   

```{r nb-urls, fig.cap = "Number of URLs per language on the left and the number of (native) speakers on the right."}
p1 <- links %>%
  ggplot(aes(y = reorder(language, language,
                     function(x)-length(x)))) +
  geom_bar() +
  ylab("Language") +
  xlab("Number of URLs") +
  theme_bw()

language_popularity <-
  tibble(language = c("English", "Hebrew", "Arabic", "Dutch", "French",
                          "German", "Italian", "Spanish", "Chinese", "Russian",
                          "Persian"),
             language_short = c("Eng", "Heb", "Ara", "Dut", "Fre", "Ger", "Ita",
                                "Spa", "Chi", "Rus", "Per"),
             first_language = c(380, 5, 0, 25, 81, 75, 67, 485, 939, 147, 72),
             second_language = c(1077, 4, 274, 5, 229, 58, 18, 74, 199, 108,
                                 38)) %>%
  mutate(total_speakers = first_language + second_language)
p2 <- ggplot(language_popularity %>%
               pivot_longer(cols = c("first_language", "second_language"),
                            names_to = "type", values_to = "number"
                            )) +
  geom_bar(aes(y = reorder(language, number,
                           function(x) -sum(x)),
               x = number, fill = type),
           stat = "identity") +
  ylab("Language") +
  xlab("Number of speakers (million)") +
  theme_bw() +
  scale_fill_discrete("",
                      breaks = c("first_language", "second_language"),
                      labels = c("First language", "Second language")) +
  theme(legend.position = c(0.7,  0.8))
p1 + p2
  
```

For each of these links, we get the page title, the body of text, and the dates that the page was first published and last edited.

```{r webscraping-get-url-content, eval = !file.exists(find_root_file("data", "content.parquet", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
if (!file.exists(
  find_root_file("data", "content.parquet",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))) {
  library(tictoc) #track duration
  tic()
  pjs_instance <- run_phantomjs()
  pjs_session <- Session$new(port = pjs_instance$port)
  future::plan("multicore", workers = 7)
  content <- furrr::future_map(links[, "link"],
                          .f = function(x) get_content(url = x))
  toc()
  links <- links %>%
    mutate(page_title = unlist(lapply(content, function(l) l[[1]])),
           page_content = unlist(lapply(content, function(l) l[[2]])),
           date_modified = unlist(lapply(content, function(l) l[[3]])),
           date_published = unlist(lapply(content, function(l) l[[4]])))

    write_parquet(x = links,
                  find_root_file("data", "content.parquet",
                                   criterion =
                                     has_file("col_an_data_for_soc_sci.Rproj")))
}
```


```{r webscraping-load-url-content, eval = file.exists(find_root_file("data", "content.parquet", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
#load the file locally if it exists
content <- read_parquet(
  find_root_file("data", "content.parquet",
                 criterion =
                   has_file("col_an_data_for_soc_sci.Rproj"))) %>%
  mutate(language = as.factor(language),
         language_short = as.factor(language_short))
``` 
# Discussion

\clearpage

\appendix

# Appendix


# Bibliography
