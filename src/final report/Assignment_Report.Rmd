---
documentclass: article
fontsize: 10pt
date: "`r Sys.Date()`"
output: 
  bookdown::pdf_document2: 
    fig_caption: yes
    toc: false
    latex_engine: xelatex
    includes:
      in_header: preamble.sty
      before_body: titlepage.sty
bibliography: references.bib  
editor_options: 
  markdown: 
    wrap: 72
  chunk_output_type: console
---

```{r setup, include=FALSE, message=FALSE, warning=FALSE, appendix=TRUE}
#---------------------------load all packages----------------------------------#
#General packages
library(tidyverse)       #Data wrangling and ggplot, also contains stringr
library(rprojroot)       #Relative file paths for reproducibility
library(kableExtra)      #Pretty "kable" tables
library(git2rdata)       #to efficiently store data on github
library(arrow)           #save and load parquet files
library(patchwork)       #combining ggplot figures
library(htmlwidgets)     #saving wordcloud
library(webshot)         #saving wordcloud
library(tidytext)
#packages for web scraping
library(webdriver)
library(rvest)
library(furrr)           #process in parallel
library(ggplot2)  
library(plotly)

#packages for languages
library(deeplr)          #for easy translations from R
library(stopwords)       #multi-lingual database with stopwords
library(tokenizers)      #for tokenization
library(tm)              #text mining in R
library(wordcloud2)       #to create a word cloud
library(syuzhet)         #for multi-lingual sentiment analysis
library(text)   # for word embeddings 

#load custom functions
source(find_root_file("src", "final report", "helperfunctions.R",
                      criterion = has_file("col_an_data_for_soc_sci.Rproj")))

knitr::opts_chunk$set(echo = FALSE, # do not show code by default.
                      appendix = TRUE) # include all code in appendix by default
# knitr::opts_knit$set(
#   root.dir = find_root(criterion = has_file("col_an_data_for_soc_sci.Rproj")))

# important option for text analysis
options(stringsAsFactors = F)
```

# Introduction

The Israeli-Palestinian conflict has its origins in the late 19$^{th}$
and early 20$^{th}$ century when Jewish settlers arrived to Ottoman
Palestine. In 1947, after the second world war, the United Nations (UN)
partition plan officially split Palestine into and Arab state, Jewish
state, and Jerusalem. Important aspects of conflict are the Israeli
occupation of the West Bank and Gaza strip, Israeli settlements (see
also the Figure below), the division of Jerusalem, borders, security and
water rights. Over the years, the conflict has escalated to actual wars
several times. The UN partition plan led to a lot of violence and, in
1948, the Arab-Israeli war. The six-day war in 1967 and the Yom-Kippur
war in 1973 were followed by the first (1987-1993) and second
(2000-2005) Intifada (@caplan2019). Currently, the conflict is back in
the news with an ongoing war between Israel and Hamas. This increase in
global attention inevitably causes the general population to inform
themselves about the conflict online, often on Wikipedia. Wikipedia
pages are typically written by a community of interested and motivated
volunteers. However, pages on the most popular/important topics
typically exist in many different languages and the content may differ
quite substantially, depending on the language.

Therefore, the research question for this analysis reads as follows:
“Are there differences in sentiment in how Wikipedia articles in
different languages describe the Israel-Palestine conflict?” This
research aims to conduct an explanatory study to see whether a sentiment
analysis of Wikipedia pages on the conflict in different languages can
unveil underlying patterns. The analysis is performed by carrying out a
sentiment analysis on translated web-scraped Wikipedia articles of 11
languages that are spoken by relevant global actors: Hebrew, Arabic,
English, Dutch, French, German, Italian, Spanish, Chinese, Russian and
Persian.

```{r territory, echo=FALSE, fig.cap= "Division of Palestinian and Israeli territory over the years (source: https://yalibnan.com/2014/06/27/eu-warns-business-israeli-settlements/)", out.width = "15cm"}
knitr::include_graphics(
  find_root_file("data", "fig", "vanishing-palestine.jpg",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

In addition to the main resesarch question, we also conduct additional experiments in this study, for example, clustering the English pages based on their similiaries and investigating the difference between the clusters in terms of their top keywords. 

## Literature review

Research in sentiment analysis has mainly been used for determining
sentiment of opinion in online reviews. Recently, researchers have
looked into mining opinion in text from news articles and blogs
(@tripathi2015sentiment). Data mining and text mining techniques are
needed to analyze people’s attitudes and emotions towards specific
topics. However, finding (political) bias in published text is a
complicated problem due to the huge text corpus of articles
(@Gupta2009FindingBI). In content analysis today, positive and negative
representation of topics are often coded manually. These approaches do
not share a research methodology due to the problem that there cannot be
an objective standard of what constitutes neutrality (@al2012automatic).
This is why sentiment analysis has become a major area of research in
the field of Natural Language Processing (NLP) and aims to determine the
attitude of a writer with respect to a specific topic
(@Kaya2012sentiment).

There are three different levels of granularity to address the problem
of a sentiment analysis: sentence-level, article-level and concept-level
sentiment analysis. A common level is the sentence level, where each
sentence has only one target concept. Article-level sentiment analysis
only targets one concept in a whole article. The concept-level sentiment
analysis includes multiple sentences about the same concept, or once
sentence that contains sentiments towards different concepts. There are
two approaches to extract concepts: manually create a list of
interesting objects before the analysis or to extract concepts from the
content automatically (@zhou2015wikipedia). For this research, Wikipedia
articles with key words regarding the Israel-Palestine conflict were
manually selected, then the sentiment score per article level were
determined.

The documents on which previous sentiment analysis has been applied
differs across research articles. Twitter has likely been the most
popular choice [@al2019multi; @imtiaz2022taking; @matalon2021using;
@ramdhani2023sentiment; @abusheikh2023digital] but web scraping from
Twitter's successor, X, has become very difficult, so alternatives had
to be considered. @al2018bias identified patterns in the press
orientation and further in the media bias towards either the Palestinian
or Israeli side from press reports and articles. Similarly,
@alsubari2024online based their research on news channel streams to
assess channel and user engagement. Other examples where opinions are
compared in different languages are @abusheikh2023digital and
@al2012automatic. The latter is of particular interest since they also
based their research on Wikipedia articles.

Wikipedia is the most widely used encyclopedia in collaborative
knowledge building, it contains more than 60 million articles and has
more than 300 languages (@wikipedia-list). The encyclopedia is committed
to what it calls a “Neutral Point of View” (NPOV), which is defined as
representing proportionately, fairly, and as far as possible without
bias. The automatic identification of POV is of high potential for
content analysis in social sciences [@al2012automatic]. However,
researchers have raised doubts about Wikipedia’s neutrality, that people
of different language backgrounds share different sources of information
and cultures. These differences have reflected on the type of
information covered [@zhou2015wikipedia]. Additionally, Wikipedia
webpages allow opinions, as long as they come for reliable authors
[@wikipedia-reliable-sources].

# Methodology

The methodology to answer the research question are as follows:

1.  Web scraping and web crawling: The automated download of HTML pages
    is called **Crawling**. The extraction of the textual data and/or
    metadata (for example, article date, headlines, author names,
    article text) from the HTML source code is called **Scraping**. We
    plan to use he R package
    [\underline{rvest}](https://cran.r-project.org/web/packages/rvest/index.html)
    for this task, inspired by two tutorials in particular
    [\underline{here}](https://ghanadatastuff.com/post/webscraping_wikipedia/)
    and
    [\underline{here}](https://github.com/tm4ss/tm4ss.github.io/blob/master/Tutorial_1_Web_scraping.Rmd).

2.  Sentiment analysis: Since we want to compare the sentiment of
    wikipedia pages in different languages, we need a tool that is able
    to perform sentiment analysis in different languages. The
    [\underline{Syuzhet}](https://cran.r-project.org/web/packages/syuzhet/vignettes/syuzhet-vignette.html)
    package currently supports many languages. The package includes four sentiment
    dictionaries and has a sentiment extraction tool that was developed
    in the NLP group at Stanford.

3.  A detailed look at the sentiments over time in the different languages.

Besides the above methodologies for investigating our main reseearch question, we also used state-of-the-art natural language processing techniques such as word embeddings, unsupervised machine learning methods such as K means clustering and PCA analysis in our additional experiment - the purpose of which was to cluster the articles based on their similarities. 

# Results

## Web scraping and crawling

We will show all code used to web scrape the relevant Wikipedia pages.
However, to efficiently run the notebook, we store and share all
web-scraped information on GitHub. This data, if available on your local
machine (pull the repository to get it), will be loaded from your hard
drive into your R environment in stead of being re-created from scratch.
This will save a significant amount of time when going through the code.

The web scraping starts from **one** Wikipedia page for each language.
Starting from the English version of "Israeli-Palestinian conflict" page
([here](https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict)),
we manually get the link to all other 10 languages of interest for this
page. Starting from these 11 URLs, we get all links to other Wikipedia
pages in the same language that are accessible from the
"Israeli-Palestinian conflict" page.

```{r webscraping-geturls, echo = FALSE, eval = !file.exists(find_root_file("data", "all_urls.yml", criterion =  has_file("col_an_data_for_soc_sci.Rproj")))}
#-------------------------------webscraping------------------------------------#
if(!file.exists(
  find_root_file("data", "all_urls.yml",
                 criterion =  has_file("col_an_data_for_soc_sci.Rproj")))){
  #webdriver::install_phantomjs() #install the [phantomJS](https://phantomjs.org)
  #headless browser. This needs to be done only once.
  #this dataframe will form the basis 
  urls <- data.frame(
    language = c("English", "Hebrew", "Arabic", "Dutch", "French", "German",
                 "Italian", "Spanish", "Chinese", "Russian", "Persian"),
    language_short = c("Eng", "Heb", "Ara", "Dut", "Fre", "Ger", "Ita", "Spa",
                       "Chi", "Rus", "Per"),
    url = c("https://en.wikipedia.org/wiki/Israeli%E2%80%93Palestinian_conflict",#Eng
          "https://he.wikipedia.org/wiki/%D7%94%D7%A1%D7%9B%D7%A1%D7%95%D7%9A_%D7%94%D7%99%D7%A9%D7%A8%D7%90%D7%9C%D7%99-%D7%A4%D7%9C%D7%A1%D7%98%D7%99%D7%A0%D7%99",#Heb
          "https://ar.wikipedia.org/wiki/%D8%A7%D9%84%D9%82%D8%B6%D9%8A%D8%A9_%D8%A7%D9%84%D9%81%D9%84%D8%B3%D8%B7%D9%8A%D9%86%D9%8A%D8%A9",#Ara
          "https://nl.wikipedia.org/wiki/Isra%C3%ABlisch-Palestijns_conflict", #Dut
          "https://fr.wikipedia.org/wiki/Conflit_isra%C3%A9lo-palestinien",#Fre
          "https://de.wikipedia.org/wiki/Israelisch-pal%C3%A4stinensischer_Konflikt", #Ger
          "https://it.wikipedia.org/wiki/Conflitto_israelo-palestinese", #Ita
          "https://es.wikipedia.org/wiki/Conflicto_israel%C3%AD-palestino", #Spa
          "https://zh.wikipedia.org/wiki/%E4%BB%A5%E5%B7%B4%E5%86%B2%E7%AA%81",#Chi
          "https://ru.wikipedia.org/wiki/%D0%9F%D0%B0%D0%BB%D0%B5%D1%81%D1%82%D0%B8%D0%BD%D0%BE-%D0%B8%D0%B7%D1%80%D0%B0%D0%B8%D0%BB%D1%8C%D1%81%D0%BA%D0%B8%D0%B9_%D0%BA%D0%BE%D0%BD%D1%84%D0%BB%D0%B8%D0%BA%D1%82", #Rus
          "https://fa.wikipedia.org/wiki/%D8%AF%D8%B1%DA%AF%DB%8C%D8%B1%DB%8C_%D9%81%D9%84%D8%B3%D8%B7%DB%8C%D9%86_%D9%88_%D8%A7%D8%B3%D8%B1%D8%A7%D8%A6%DB%8C%D9%84"#Per
          ))
  links <- lapply(X = seq_len(nrow(urls)),
                  FUN = function(x) get_all_links(
                    language = urls[x, "language"],
                    language_short = urls[x, "language_short"],
                    url = urls[x, "url"]
                  ))
  links <- bind_rows(links) %>%
    mutate(language = as.factor(language),
           language_short = as.factor(language_short))
  #save the urls such that this data can be loaded locally during knitting.
  write_vc(x = links,
           file = "all_urls",
           root = find_root_file("data",
                                 criterion =
                                   has_file("col_an_data_for_soc_sci.Rproj")))
}
```

```{r webscraping-geturls-load, eval = file.exists(find_root_file("data", "all_urls.yml", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
#load the webscraped content file locally if it exists
links <- read_vc(
  file = "all_urls",
  root = find_root_file("data",
                        criterion =
                          has_file("col_an_data_for_soc_sci.Rproj"))) %>%
  mutate(language = as.factor(language),
         language_short = as.factor(language_short))
```

In total, this search yields `r nrow(links)` URLs for all 11 languages.
Figure \@ref(fig:nb-urls) shows the number of linked Wikipedia pages for
each of the languages on the left. For comparison, we show the number of
(native) speaker on the right since the number of Wikipedia pages will
depend a lot on the number of speakers who can write the pages. Taking
both into account, we can see that Arabic and English have the most
Wikipedia pages linked to the "Israeli-Palestinian conflict" page.
Arabic and Hebrew, logically, have much more Wikipedia pages on the
issue than what would be expected, given their lower number of native
speakers.

```{r nb-urls, fig.cap = "Number of URLs per language on the left and the number of (native) speakers on the right.", warning = FALSE, message = FALSE}
#--------------------------plot number of urls per language--------------------#
p1 <- links %>%
  ggplot(aes(y = reorder(language, language,
                     function(x)-length(x)))) +
  geom_bar() +
  ylab("Language") +
  xlab("Number of URLs") +
  theme_bw()

language_popularity <-
  tibble(language = c("English", "Hebrew", "Arabic", "Dutch", "French",
                          "German", "Italian", "Spanish", "Chinese", "Russian",
                          "Persian"),
             language_short = c("Eng", "Heb", "Ara", "Dut", "Fre", "Ger", "Ita",
                                "Spa", "Chi", "Rus", "Per"),
             first_language = c(380, 5, 0, 25, 81, 75, 67, 485, 939, 147, 72),
             second_language = c(1077, 4, 274, 5, 229, 58, 18, 74, 199, 108,
                                 38)) %>%
  mutate(total_speakers = first_language + second_language)
p2 <- ggplot(language_popularity %>%
               pivot_longer(cols = c("first_language", "second_language"),
                            names_to = "type", values_to = "number"
                            )) +
  geom_bar(aes(y = reorder(language, number,
                           function(x) -sum(x)),
               x = number, fill = type),
           stat = "identity") +
  ylab("Language") +
  xlab("Number of speakers (million)") +
  theme_bw() +
  scale_fill_discrete("",
                      breaks = c("first_language", "second_language"),
                      labels = c("First language", "Second language")) +
  theme(legend.position = c(0.65,  0.8))
p1 + p2
  
```

For each of these links, we get the page title, the body of text, and
the dates that the page was first published and last edited.

```{r webscraping-get-url-content, eval = !file.exists(find_root_file("data", "content.parquet", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
#----------------------------webscrape all website content---------------------#
if (!file.exists(
  find_root_file("data", "content.parquet",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))) {
  library(tictoc) #track duration
  tic()
  pjs_instance <- run_phantomjs()
  pjs_session <- Session$new(port = pjs_instance$port)
  future::plan("multicore", workers = 7)
  content <- furrr::future_map(links[, "link"],
                          .f = function(x) get_content(url = x))
  toc()
  links <- links %>%
    mutate(page_title = unlist(lapply(content, function(l) l[[1]])),
           page_content = unlist(lapply(content, function(l) l[[2]])),
           date_modified = unlist(lapply(content, function(l) l[[3]])),
           date_published = unlist(lapply(content, function(l) l[[4]])))

    write_parquet(x = links,
                  find_root_file("data", "content.parquet",
                                   criterion =
                                     has_file("col_an_data_for_soc_sci.Rproj")))
}
```

```{r webscraping-load-url-content, eval = file.exists(find_root_file("data", "content.parquet", criterion =     has_file("col_an_data_for_soc_sci.Rproj")))}
#load the file locally if it exists
content <- read_parquet(
  find_root_file("data", "content.parquet",
                 criterion =
                   has_file("col_an_data_for_soc_sci.Rproj"))) %>%
  mutate(language = as.factor(language),
         language_short = as.factor(language_short))
```

## Filtering relevant web pages

Looking at the scraped pages, it is clear that some web pages are not
relevant for the Israeli-Palestinian conflict. This was to be expected
since Wikipedia has the tendency to link to *all* topics for which a
Wikipedia page exists. Before we start the time-consuming steps of
removing stop words and preparing the web pages for language models, we
therefore defined a fixed set of 10 keywords and we will count how many
times these appear in the text (Table \@ref(tab:filter-keywords)). The
keywords are translated in all languages using the *deeplr* package
which allows us to translate up to 500,000 characters per month with a
free DeepL account. The package did not support Persian, Arab, and
Hebrew so those languages were translated manually, using google translate.

```{r eval = !file.exists(find_root_file("data", "filtering_keywords.yml", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
#--------------------------Filtering relevant web pages------------------------#
# Since an authentication key is needed to perform the translation,
# #we save the translated keywords to github and load them from there.
if (!file.exists(find_root_file("data", "filtering_keywords.yml",
                               criterion =
                               has_file("col_an_data_for_soc_sci.Rproj")))) {
authentication_key <- "secret"
keywords <-
  data.frame(English = c("israel", "israeli", "palestine", "palestinian",
                         "arab", "gaza", "intifada", "jewish", "zionism",
                         "hamas"))
keywords <- keywords %>%
  mutate(Dutch = translate2(target_lang = "NL", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        French = translate2(target_lang = "FR", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        German = translate2(target_lang = "DE", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        Italian = translate2(target_lang = "IT", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        Spanish = translate2(target_lang = "ES", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        Chinese = translate2(target_lang = "ZH", source_lang = "EN",
                             text = English, auth_key = authentication_key),
        Russian = translate2(target_lang = "RU", source_lang = "EN",
                             text = English, auth_key = authentication_key)
        )
keywords$Arabic <- c("إسرائيل", "إسرائيلي", "فلسطين", "فلسطيني", "عربي", "غزة", "الانتفاضة", "يهودي", "صهيونية","حماس")
keywords$Hebrew <- c("ישראל", "ישראלי", "פלסטין", "פלסטיני", "ערבי", "עזה", "אנטיפדה", "יהודי", "ציונות", "חמאס")
keywords$Persian <- c("رژیم صهیونیستی", "اسرائیلی", "فلسطین", "فلسطینی", "عربی", "غزه", "انتفاضه", "یهودی", "صهیونیسم", "حماس")
git2rdata::write_vc(
  keywords,
  file = "filtering_keywords",
  root = find_root_file("data",
                        criterion = has_file("col_an_data_for_soc_sci.Rproj")))
}
```

```{r filter-keywords}
keywords <- git2rdata::read_vc(
  file = "filtering_keywords",
  root = find_root_file("data",
                        criterion = has_file("col_an_data_for_soc_sci.Rproj")))
keywords %>%
  dplyr::select(English) %>%
  kable(booktabs = TRUE,
        caption = "Keywords used to filter the scraped web pages") %>%
  kableExtra::kable_styling()

```

```{r eval = !file.exists(find_root_file("data", "keyword_frequency.yml", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
#calculate the total number of words and the number of times each keyword appears
if (!file.exists(
  find_root_file("data", "keyword_frequency.yml",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))) {
nbs <-
  data.frame(nchar = str_length(content$page_content),
             israel = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[1,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             israeli = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[2,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             palestine = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[3,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             palestinian = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[4,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             arab = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[5,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             gaza = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[6,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             intifada = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[7,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             jewish = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[8,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             zionism = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[9,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }),
             hamas = sapply(X = seq_len(nrow(content)),
                             FUN = function(x){
                               key <- keywords[10,
                                               as.character(
                                                 unlist(
                                                   content[x, "language"]))]
                               return(str_count(
                                 content[x, "page_content"],
                                 regex(key, ignore_case = T)))
                               }))
nbs <- nbs %>%
  mutate(
    link = content$link,
    language = content$language,
    total = israel + israeli + palestine + palestinian + arab + gaza +
      intifada + jewish + zionism + hamas,
    totalperchar = total/nchar)
git2rdata::write_vc(nbs,
                    file = "keyword_frequency",
                    root = find_root_file("data",
                                 criterion =
                                   has_file("col_an_data_for_soc_sci.Rproj")))
}
```

```{r keyword-frequency, fig.cap = "Overview of the keyword frequency. X-axis is bound at 14 since this is the 90percent quantile for keyword frequency and there are many ouliers.", warning = FALSE, message = FALSE}
#load the file locally if it exists
nbs <- read_vc(file = "keyword_frequency",
               root =
                 find_root_file("data",
                                criterion =
                                  has_file("col_an_data_for_soc_sci.Rproj")))
#get quantiles 
quantiles <- nbs %>%
    pivot_longer(cols = israel:hamas, names_to = "keyword",
                 values_to = "frequency") %>%
  dplyr::pull(frequency) %>%
  quantile(probs = seq(0,1,0.05))
 
nbs %>%
  pivot_longer(cols = israel:hamas, names_to = "keyword",
               values_to = "frequency") %>%
  dplyr::filter(frequency <= 14) %>%
  ggplot() +
  geom_histogram(aes(x = frequency)) +
  facet_wrap(vars(keyword)) +
  theme_bw() +
  ylab("number of web pages") +
  xlab("number of times the keyword appears")
```

```{r total-keywords, fig.cap="Total number of times any of the characters appears in the web page (on the left) and corrected for the length of the web page (on the right). Outliers to the right are not shown.", warning = FALSE, message = FALSE, fig.width = 8}
#Plot the number of times the keywords appears
p1 <- nbs %>%
  ggplot(aes(x = total)) +
  geom_histogram() +
  xlim(0, 800) +
  ylim(0, 2000) +
  theme_bw() +
  xlab("number of times any of the keywords appears") +
  ylab("number of web pages")
p2 <- nbs %>%
  ggplot(aes(x = totalperchar)) +
  geom_histogram() +
  xlim(0, 0.05) +
  ylim(0, 1000) +
  theme_bw() +
  xlab("number of times any of the keywords appears,\n divided by the number of characters") +
  ylab("number of web pages")

p1 + p2

quantiles <- nbs %>%
  dplyr::pull(totalperchar) %>%
  quantile(probs = seq(0,1,0.05), na.rm = TRUE)
a <- (nbs$total > 5 & nbs$totalperchar > 0.00121)
keep_total <- sum(a)
```

```{r write-filtered-web-pages, eval = !file.exists(find_root_file("data", "content-filtered.parquet", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
content_filtered <- content %>%
                dplyr::filter(link %in% nbs[a, "link"])
#save the filtered web pages for further data exploration
write_parquet(x = content_filtered,
                  find_root_file("data", "content-filtered.parquet",
                                   criterion =
                                     has_file("col_an_data_for_soc_sci.Rproj")))

```

```{r read-filtered-web-pages, eval = file.exists(find_root_file("data", "content-filtered.parquet", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
content_filtered <- read_parquet(
  find_root_file("data", "content-filtered.parquet",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

Figure \@ref(fig:keyword-frequency) shows how often each of the keywords
appear in the web pages. We will sum the frequency for each of the
keywords to obtain the total number of time *any* of the keywords appear
in each of the web pages (see the Figure \@ref(fig:total-keywords) on
the left). To correct for the length of a web page, we divide this total
by the number of characters in the web page (see the Figure
\@ref(fig:total-keywords) on the right).

In over 15% of the web pages, none of the keywords appear. These web
pages are for sure irrelevant. Since we only want to keep web pages that
are relevant for our topic of interest, we will only keep web pages
where any of the keywords appear at least 5 times and the total keyword
frequency, divided by the number of characters is at least 0.00121 (that
is the 30% quantile). In total, this reduces out dataset from
`r nrow(content)` to `r keep_total`
(`r round(100*keep_total/nrow(content), 2)`%) web pages.

Figure \@ref(fig:keep-per-language) shows that relatively many Persian
web pages are filtered out. Upon further inspection, it was clear that
the Persian word for Israel, اسرائيل, could be found in exactly zero web
pages. Native speakers could tell us that, since Iran does not recognise
Israel as a country, they call it "zionist regime",رژیم صهیونیستی , or
"occupied Palestine", فلسطین اشغالی in stead. Since "zionist regime" had
the most hits, we used that term in stead but it still yielded far less
matches than the term "Israel" got in any other language.

```{r keep-per-language, fig.cap = "Number of web pages that are filtered out for each of the languages in absolute numbers (left) and in percentages (right)."}
#plot how many web pages remain after filtering, per language
p1 <- nbs %>%
  mutate(keep = total > 5 & totalperchar > 0.00121) %>%
  ggplot(aes(x = language)) +
  geom_bar(aes(fill = keep)) +
  theme_bw() +
  ylab("number of web pages") +
  coord_flip()
p2 <- nbs %>%
  mutate(keep = total > 5 & totalperchar > 0.00121) %>%
  ggplot(aes(x = language)) +
  geom_bar(aes(fill = keep), position = "fill") +
  theme_bw() +
  ylab("fraction of web pages") +
  xlab("") +
  coord_flip()
combined <- p1 + p2 & theme(legend.position = "bottom")
combined + plot_layout(guides = "collect")
```

## Text pre-processing

The Wikipedia page content contains a lot of redundant characters like
references to other web pages (in the form of a number between square
brackets) and line breaks. Special care was taken since numbers in the
Persian language are not the commonly used arabic numerals.

Next, the R package *tokenizers* is used to tokenize the cleaned content
into sentences (see also the [package
documentation](https://cran.r-project.org/web/packages/tokenizers/vignettes/introduction-to-tokenizers.html)).
Using the *tm* package, we then bring all text to lower case, remove
punctuation, remove numbers, remove stopwords. We use the R package
*stopwords* to filter stopwords from the text since this package
contains stopwords in many languages which is a must for our analysis
(see also the [package
documentation](https://cran.r-project.org/web/packages/stopwords/readme/README.html)).
Finally, a document-term matrix is generated for each of the languages.
These document-term matrices are used in appendix \@ref(wordclouds) to
create a word cloud for each of the languages.

```{r warning = FALSE, message = FALSE, eval = !file.exists(find_root_file("data", "dtm.Rdata", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
#-----------------------------text pre-processing------------------------------#
if (!file.exists(
  find_root_file("data", "dtm.Rdata",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))) {
content_filtered <-
  content_filtered %>%
  mutate(page_content = str_replace_all(page_content, "\n", " "), #page breaks
         page_content = str_replace_all(page_content,
                                        "\\[[\u06F0-\u06F90-9]+\\]", ""),
         #references with arabic numerals or persian numerals
         page_content = str_replace_all(page_content, "[\"']", ""),
         page_content = str_squish(page_content)
         )
# tokenize the cleaned content into sentences
content_filtered <- content_filtered %>%
  mutate(tokenized_content =
           sapply(content_filtered$page_content,
                  function(content) {
                    sentences <- unlist(tokenize_sentences(content))
                    return(sentences)
                    })
         )
#a function to get the document-term matrix for a language
corpus_dtm <- function(lang, language_short, src){
  data <- content_filtered %>% filter(language == lang)
  stop_words <- stopwords::stopwords(language = language_short, source = src)
  corpus <- Corpus(VectorSource(data$tokenized_content)) %>%
    tm_map(content_transformer(tolower)) %>%
    tm_map(removePunctuation) %>%
    tm_map(removeNumbers) %>%
    tm_map(removeWords, stop_words) %>%
    tm_map(stripWhitespace)
  dtm <- DocumentTermMatrix(corpus)
  return(dtm)
}
loopover <-
  data.frame(lang = unique(as.character(content$language)),
             language_short = c("en", "he", "ar", "nl", "fr", "de", "it", "es",
                                "zh", "ru", "fa"),
             source = c("snowball", "stopwords-iso", "misc", "stopwords-iso",
                        "stopwords-iso", "stopwords-iso", "stopwords-iso",
                        "stopwords-iso", "stopwords-iso", "stopwords-iso",
                        "stopwords-iso"))
dtm <- furrr::future_map(seq_len(nrow(loopover)),
                          .f = function(x) corpus_dtm(
                            lang = loopover[x, "lang"],
                            language_short = loopover[x, "language_short"],
                            src = loopover[x, "source"]))
names(dtm) <- loopover$lang
save(dtm, file = find_root_file("data", "dtm.Rdata",
                      criterion = has_file("col_an_data_for_soc_sci.Rproj")))
}
```

```{r warning = FALSE, message = FALSE, eval = file.exists(find_root_file("data", "dtm.Rdata", criterion = has_file("col_an_data_for_soc_sci.Rproj")))}
#load dtm locally if it exists
load(find_root_file("data", "dtm.Rdata",
                      criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

## Sentiment analysis per page

Figure \@ref(fig:hist-sentiment) shows the sentiment score for each of
the languages. Although the *syuzhet* package provides support for sentiment
detection in several languages by using the expanded NRC lexicon from
Saif Mohammed, languages that use non-Latin character sets are not yet
implemented in the current release which can be seen in the histogram
since almost all scores are equal to zero in those languages.

```{r cache = TRUE, warning = FALSE, message = FALSE}
# sentiment analysis for each page (language independent)
content_filtered <- content_filtered %>%
  mutate(
    sentiment_score = get_sentiment(page_content, method = "syuzhet")
  )

```

```{r hist-sentiment, fig.cap = "Histogram of the sentiment score for all documents in each of the languages.", fig.height = 8, warning = FALSE, message = FALSE}
content_filtered %>%
  ggplot() +
  geom_histogram(aes(x = sentiment_score)) +
   facet_wrap(vars(language),
              scales = "free",
              ncol = 4) +
  theme_bw() +
  ylab("Number of documents")
```

Nevertheless, we can visualise the sentiment in the Wikipedia pages over
time. Figure \@ref(fig:sent-date-published) and Figure
\@ref(fig:sent-date-modified) show the sentiment in the Wikipedia pages
over time, based on the date that the page was first published or last
modified respectively. As mentioned earlier, languages with non-latin
characters like Chinese and Russian do not show any clear sentiment
scores over time. From these figures, it can be concluded that there is
a clear difference of sentiment across languages. Languages like Dutch
and Spanish seem to carry a negative sentiment in articles about Israel
and Palestine form 2020 onwards. A huge outlier in figure
\@ref(fig:sent-date-published) is the changing of the sentiment score of
German after 2015. Figure \@ref(fig:sent-date-modified) highlights the
start of the Israel-Hamas war on October 7 2023, but there does not seem
to be a clear change in sentiment for any language.

```{r sent-date-published, fig.cap="Sentiment over time in different languages, based on the date that the page was first published.", warning = FALSE, message = FALSE}
# Data
content_time <- content_filtered %>%
  mutate(
    date_published = as.Date(date_published)
  )

# Colourcoding
language_colors <- c("Arabic" = "#1f77b4",  
                     "Chinese" = "#ff7f0e", 
                     "Dutch" = "#2ca02c",   
                     "English" = "#9467bd", 
                     "German" = "#8c564b", 
                     "Hebrew" = "#e377c2", 
                     "Italian" = "#bcbd22",  
                     "Persian" = "#17becf", 
                     "Russian" =  "#aec7e8", 
                     "Spanish" =  "#ff9896"
                    )

# Plotting
start_date <- as.Date("2005-01-01")
end_date <- as.Date("2024-05-31")

ggplot(content_time, aes(x = date_published, y = sentiment_score,
                         color = language)) +
  geom_smooth(method = "loess", se = FALSE, size = 0.75) + 
  scale_color_manual(values = language_colors) + 
  scale_x_date(limits = c(start_date, end_date)) +
  scale_y_continuous(limits = c(-1, 1)) +
  labs(x = "Date Published", y = "Sentiment Score",
       color = "Language") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom")  
```

```{r sent-date-modified, fig.cap="Sentiment over time in different languages, based on the date that the page was last modified.", warning = FALSE, message = FALSE}
content_date_modified <- content_filtered %>%
  mutate(
    date_modified = as.Date(date_modified)
  )

# Setting the range of the plot + plotting
start_date <- as.Date("2023-05-31")
end_date <- as.Date("2024-05-31")

ggplot(content_date_modified, aes(x = date_modified, y = sentiment_score, color = language)) +
  geom_smooth(method = "loess", se = FALSE, size = 1) +  
  geom_vline(xintercept = as.numeric(as.Date("2023-10-07")), color = "red") + #Day that the Israel-Hamas war began
   scale_color_manual(values = language_colors) + 
  scale_x_date(limits = c(start_date, end_date)) +  
  scale_y_continuous(limits = c(-0.5, 0.5)) +  
  labs(x = "Date Published", y = "Sentiment Score", color = "Language") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1),
        legend.position = "bottom") 

```

We can also visualize the sentiment categories as predicted by the
syuzhet package, using the NRC sentiment dictionary. This calculates the
presence of eight emotions in the texts. Figure \@ref(fig:nrc-posneg),
\@ref(fig:nrc-by-cat), and \@ref(fig:nrc-avg-sent) show the results of
this analysis. Figure \@ref(fig:nrc-posneg) shows that English has
both the highest mean score for positive and negative sentiment,
followed by French, German and Dutch. Figure \@ref(fig:nrc-by-cat)
breaks down sentiment in 7 emotions categories that are part of the
*syuzhet* package: anticipation, disgust, fear, joy, sadness, surprise
and trust. Figure \@ref(fig:nrc-avg-sent) compares the average *emotion*
categories within each language, this allows us to draw interesting
conclusions. Trust seems to the most common emotional category for all,
but this seems to be the only similarity between the languages. The
'Fear' sentiment seems to be comparatively high for Dutch, English,
French and German. 'Sadness' seems to be comparatively high for Russian,
and 'Joy' seems to be high for Hebrew.

```{r nrc-posneg, cache = TRUE, fig.cap = "Positive and Negative Sentiment of Langauges.", warning = FALSE, message = FALSE}
nrc_data <- content_filtered %>%
  mutate(page_content = as.character(page_content)) %>%
  mutate(language = as.factor(language)) %>%
  group_by(language) %>%
  mutate(sentiment_scores = get_nrc_sentiment(page_content)) %>%
  ungroup()

nrc_data <- nrc_data %>%
  unnest(sentiment_scores)

# Positive or Negative Sentiment
sentiment_pos_neg <- c("negative", "positive")

# Calculate mean sentiment scores by language
sentiment_pos_neg_summary <- nrc_data %>%
  group_by(language) %>%
  summarise(across(all_of(sentiment_pos_neg), mean, na.rm = TRUE))

# Reshape data for plotting
sentiment_pos_neg_convert <- tidyr::pivot_longer(sentiment_pos_neg_summary, 
                                             cols = -language, 
                                             names_to = "sentiment_category", 
                                             values_to = "mean_score")

# Plotting
ggplot(sentiment_pos_neg_convert,
       aes(x = sentiment_category, y = mean_score,
           fill = reorder(language, mean_score, FUN = function(x) -mean(x)))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_color_manual(values = language_colors) + 
  facet_wrap(~ sentiment_category, scales = "free") +  
  labs(x = "Sentiment Category", y = "Mean Sentiment Score",
       fill = "Language") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

```{r nrc-by-cat, cache = TRUE, fig.cap = "Sentiment Categories of Languages.", warning = FALSE, message = FALSE}
# List of sentiment categories
sentiment_categories <- c("anticipation", "disgust", "fear", "joy", "sadness",
                          "surprise", "trust")

# Calculate mean sentiment scores by language
sentiment_summary <- nrc_data %>%
  group_by(language) %>%
  summarise(across(all_of(sentiment_categories), mean, na.rm = TRUE))

# Reshape data for plotting 
sentiment_summary_long <- tidyr::pivot_longer(sentiment_summary, 
                                             cols = -language, 
                                             names_to = "sentiment_category", 
                                             values_to = "mean_score")

# Plotting
ggplot(sentiment_summary_long,
       aes(x = sentiment_category, y = mean_score,
           fill = reorder(language, mean_score,
                          FUN = function(x) -mean(x)))) +
  geom_bar(stat = "identity", position = "dodge") +
  scale_color_manual(values = language_colors) + 
  facet_wrap(~ sentiment_category, scales = "free") +  
  labs (y = "Mean Sentiment Score", fill = "Sentiment Category") +
  theme_minimal()

```

```{r nrc-avg-sent, cache = TRUE, fig.cap = "Average sentiment per language.", warning = FALSE, message = FALSE}
ggplot(sentiment_summary_long,
       aes(x = fct_reorder(sentiment_category, mean_score),
           y = mean_score, fill = sentiment_category)) +
  geom_bar(stat = "identity", position = "dodge") +
  facet_wrap(~ language, scales = "free") +  
  labs(x = "Sentiment Category", y = "Mean Sentiment Score",
       fill = "Sentiment Category") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

## Clustering based on word embeddings and PCA

Another experimentation we did was to cluster the articles based on their similarities, with the focus on English pages. This is based on the assumption that some pages are more similar in content to each other, and we were interested in understanding whether it is possible to capture their similarities only based on the word embeddings of the top keywords from the articles. In order to do this, we use the R package *text*. The textEmbed() function gives the pre-trained word embeddings retrieved from Hugging Face transformers language models, which is the state-of-art for natural language processing. The word embedding is a vector that captures the semantic of the words, and similar words have less Euclidean distance with each other.  

Process wise, we first identify the top 10 keywords from each article. Similarly, cleaning steps such as stopwords removal is important before calculating document term matrix to ensure that we only keep the more meaningful words. Then, we calculate the average embeddding for each article using these keywords. In this way, we obtain an embedding for each article, and this is a compact representation for the semantic meaning of the article. Next we perform K-means clustering for all English pages where K is chosen to be 3. For visualization of the clusters, we also performed PCA, such that the PCA scores for each page as well as the cluster each page belongs to can be shown in two dimensions, as in Figure \@ref(fig:embedding-analysis-english). To see how the different clusters may be related to different types of contents, we then extract top 10 keywords and their frequencies for each cluster, and visualize this in Figure \@ref(fig:visualize-top-keywords-cluster). As we see, cluster 1 contains high-frequency words such as "gaza" and "war", which are missing from the second and third cluster. This implies that cluster 1 represents more of the Gaza–Israel conflict, a localized part of the Israeli–Palestinian conflict, whereas cluster 2 and 3 are more about the Israeli–Palestinian conflict in general.

```{r embedding-analysis-english, cache = TRUE, fig.cap = "Clustering/PCA on the embeddings.", warning = FALSE, message = FALSE}

library(ggplot2)  # For visualization (optional)
library(text)

# Define the function to get the top 10 keywords used for the embedding
keyWords <- function(text) {
  corpus <- Corpus(VectorSource(text))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stemDocument)
  corpus <- tm_map(corpus, stripWhitespace)
  dtm <- DocumentTermMatrix(corpus)
  dtm_matrix <- as.matrix(dtm)
  term_freq <- colSums(dtm_matrix)
  top_terms_freq <- sort(term_freq, decreasing = TRUE)
  top_keywords_freq <- names(top_terms_freq)[1:10]
  return(top_keywords_freq)
}

# Define the function to calculate the average embedding for each article based on 10 keywords
aveEmbedding <- function(keywords) {
  E <- textEmbed(keywords)
  return(colMeans(E$texts$texts))
}

# Calculate the average embeddings and save into df
# K <- keyWords(filtered_data$page_content[1])
# E = aveEmbedding(K)

# embedding_vectors <- list()
# for (i in 1:nrow(filtered_data)) {
#  K <- keyWords(filtered_data$page_content[i])
#  E = aveEmbedding(K)
#  embedding_vectors <- c(embedding_vectors, list(E))
#}

# df <- do.call(data.frame, embedding_vectors)
# df <- t(df)
# write.csv(df, "embedding_data.csv", row.names = FALSE)

# The codes to generate embeddings are very computationally heavy, so we read directly what's been generated before
df <- read.csv('embedding_data.csv')

# Perform k-means clustering
num_clusters <- 3  
kmeans_model <- kmeans(df, centers = num_clusters)
pca_result <- prcomp(df, center = TRUE, scale. = TRUE)

# Get the principal component scores
pca_scores <- pca_result$x[, 1:2]
# Create a data frame combining PCA scores and cluster assignments
pca_data <- data.frame(PC1 = pca_scores[, 1], PC2 = pca_scores[, 2], Cluster = as.factor(kmeans_model$cluster))

# Plot using ggplot2
ggplot(pca_data, aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point() +
  labs(title = "PCA Plot after K-Means Clustering",
       x = "Principal Component 1", y = "Principal Component 2", z = "Principal Component 3") +
  scale_color_discrete(name = "Cluster")

```

```{r visualize-top-keywords-cluster, cache = TRUE, fig.cap = "Word frequencies across clusters.", warning = FALSE, message = FALSE}

filtered_data$Cluster <- kmeans_model$cluster
# Select data for each cluster
cluster_1_data <- filtered_data[filtered_data$Cluster == 1, ]
cluster_2_data <- filtered_data[filtered_data$Cluster == 2, ]
cluster_3_data <- filtered_data[filtered_data$Cluster == 3, ]

keyWords20_frequency <- function(text) {
  corpus <- Corpus(VectorSource(text))
  
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, removeWords, stopwords("english"))
  corpus <- tm_map(corpus, stemDocument)
  corpus <- tm_map(corpus, stripWhitespace)
  
  dtm <- DocumentTermMatrix(corpus)
  dtm_matrix <- as.matrix(dtm)
  term_freq <- colSums(dtm_matrix)
  top_terms_freq <- sort(term_freq, decreasing = TRUE)
  top_keywords_freq <- names(top_terms_freq)[1:10]
  top_freq <- top_terms_freq[1:10]
  return(list(top_keywords_freq,top_freq))
}

K1 = keyWords20_frequency(cluster_1_data$page_content)
K2 = keyWords20_frequency(cluster_2_data$page_content)
K3 = keyWords20_frequency(cluster_3_data$page_content)

elements1 <- unlist(K1[1])
freq1 <- unlist(K1[2])

elements2 <- unlist(K2[1])
freq2 <- unlist(K2[2])

elements3 <- unlist(K3[1])
freq3 <- unlist(K3[2])

par(mfrow = c(1, 3))
barplot(freq1, names.arg = elements1, col = "skyblue",
        main = "Cluster 1",
        xlab = "", ylab = "Frequency", las = 2)

barplot(freq2, names.arg = elements2, col = "skyblue",
        main = "Cluster 2",
        xlab = "", ylab = "Frequency", las = 2)

barplot(freq3, names.arg = elements3, col = "skyblue",
        main = "Cluster 3",
        xlab = "", ylab = "Frequency", las = 2)

```

\clearpage

# Discussion

In this analysis, the differences in sentiment in Wikipedia articles on
the Israel-Palestine conflict were analysed by comparing different
languages. After web-scraping, filtering and pre-processing the text, a
sentiment analysis was run on the text. For most languages, the
Wikipedia articles seemed to reproduce an average neutral sentiment
score. However, German and French seems to skew a bit to the negative
sentiment side. When looking at the sentiment score over time, German
seems to be the main outlier with a very negative score in 2012, which
slowly became positive the years following. There was no clear effect in
any language for sentiment after the start of the Israel-Hamas conflict in
October 2023. When looking at emotional categories within sentiment,
German seems to have a high mean sentiment score for 'Fear'; Hebrew for
'Joy' and Russian for 'Sadness'. However, it is important to add that
these results have to be combined with further research and manual
coding for it to be reliable.

One of the limitations to our research is that languages that use
non-Latin alphabets were not included in the release of the *syuzhet*
package. This has not allowed us to analyse these languages or interpret
their results correctly. Another limitation of the analysis is that we
do not speak all the languages used in this dataset, this has resulted
in us not being able to pre-process the webscraped data to the best of
its abilities. However, as far as we are aware, this is a
first attempt at compiling a big exploratory study comparing sentiment
across languages on Wikipedia using NLP.

\clearpage

\appendix


# Appendix

## Wordclouds

```{r}
included_term_frequency <- 0.95
max_terms <- 500
create_wordcloud <- function(lang = "English",
                             included_term_frequency = included_term_frequency,
                             max_terms = max_terms) {
  d <- dtm[[lang]]
  freqr <- colSums(as.matrix(d))
  if ((1-included_term_frequency) * length(freqr) > max_terms){
    #choose only the top *max_terms* terms
    qt_d <- quantile(freqr, probs = (1-max_terms/length(freqr)))
    d2 <- as.matrix(d)[, freqr > qt_d]
    freqr <- colSums(d2)
  } else {
    #get the top (1-included_term_frequency) terms
    qt_d <- quantile(freqr, probs = included_term_frequency)
    d2 <- as.matrix(d)[, freqr > qt_d]
    freqr <- colSums(d2)
  }
  # p <- wordcloud(names(freqr), freqr,
  #                colors = brewer.pal(4, "Dark2"))
  p <- wordcloud2(data = data.frame(word = names(freqr),
                               freq = freqr))
  return(p)
}
if (!file.exists(find_root_file("wordcloud_Dutch.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))) {
  for (lang in names(dtm)) {
    p <- create_wordcloud(lang = lang,
                   included_term_frequency = included_term_frequency,
                   max_terms = max_terms)
    saveWidget(p, "tmp.html", selfcontained = F)
    webshot("tmp.html",
            sprintf("wordcloud_%s.pdf", lang),
            delay = 5, vwidth = 480, vheight = 480)
  }
}
```

In this section, we present word clouds based on the document-term
matrices for each of the languages. To avoid over-cluttering the
wordcloud, we only include the `r (1 - included_term_frequency)*100`% of
terms that appear most often in that language, with a maximum of
`r max_terms`.

The word clouds reveal a couple of interesting shortcomings in the text
pre-processing. Firstly, stopword removal did not work well for French since
article words like "les" and "des" were not removed properly. Chinese was not
processed well either since the word cloud mainly shows a couple of English
words and some Chinese "words" are suspiciously long.

```{r wordcloud-eng, fig.cap= "Wordcloud for the English Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_English.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-he, fig.cap= "Wordcloud for the Hebrew Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Hebrew.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-ar, fig.cap= "Wordcloud for the Arabic Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Arabic.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-du, fig.cap= "Wordcloud for the Dutch Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Dutch.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-fr, fig.cap= "Wordcloud for the French Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_French.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-de, fig.cap= "Wordcloud for the German Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_German.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))

```

```{r wordcloud-it, fig.cap= "Wordcloud for the Italian Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Italian.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))

```

```{r wordcloud-sp, fig.cap= "Wordcloud for the Spanish Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Spanish.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))

```

```{r wordcloud-chi, fig.cap= "Wordcloud for the Chinese Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Chinese.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))

```

```{r wordcloud-ru, fig.cap= "Wordcloud for the Russian Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Russian.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))
```

```{r wordcloud-fa, fig.cap= "Wordcloud for the Persian Wikipedia pages.", warning = FALSE, message = FALSE}
knitr::include_graphics(
  find_root_file("wordcloud_Persian.pdf",
                 criterion = has_file("col_an_data_for_soc_sci.Rproj")))

```

\clearpage

## All code for this report

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```

# Bibliography
